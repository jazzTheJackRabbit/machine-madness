{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn import svm, grid_search\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = \"../../\"\n",
    "dataset = pd.read_csv(ROOT_DIR+\"data/structured/training_data_match_statistics.csv\")\n",
    "dataset = dataset.drop(dataset.columns[0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "<class 'sklearn.linear_model.logistic.LogisticRegression'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn import svm, grid_search\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# \n",
    "# Creates MATCH statistics for all tournament matches in a season (for all seasons)\n",
    "# \n",
    "\n",
    "ROOT_DIR = \"../../\"\n",
    "dataset = pd.read_csv(ROOT_DIR+\"data/structured/training_data_match_statistics.csv\")\n",
    "dataset = dataset.drop(dataset.columns[0],axis=1)\n",
    "\n",
    "def getPredictionsFromClassifier(classifier,training_X,training_Y,testing_X,testing_Y):\n",
    "    # Training\n",
    "    classifier.fit(training_X,training_Y)\n",
    "    predicted_Y = classifier.predict(testing_X)\n",
    "    predicted_probabilities_Y = classifier.predict_proba(testing_X)[:,1]\n",
    "\n",
    "#     print(classification_report(predicted_Y.round().astype(int),testing_Y))\n",
    "#     print(accuracy_score(predicted_Y.round().astype(int),testing_Y))\n",
    "\n",
    "    return predicted_Y,predicted_probabilities_Y\n",
    "\n",
    "def classNameForClassifier(classifier):\n",
    "    class_name = str(classifier.__class__)\n",
    "    class_name = class_name[class_name.rfind(\".\")+1:class_name.rfind(\"'>\")]\n",
    "    return class_name\n",
    "\n",
    "def addPredictionAndClassifierToList(prediction_probabilities_for_winning,classifier,predicted_probabilities_Y):\n",
    "    class_name = classNameForClassifier(classifier)\n",
    "    prediction = pd.DataFrame(predicted_probabilities_Y,columns=[class_name])\n",
    "    prediction_probabilities_for_winning = pd.concat([prediction_probabilities_for_winning,prediction],axis=1)\n",
    "    return prediction_probabilities_for_winning\n",
    "\n",
    "# Dataset Partitioning\n",
    "dataset = pd.read_csv(ROOT_DIR+\"data/structured/training_data_match_statistics.csv\")\n",
    "dataset = dataset.drop(dataset.columns[0],axis=1)\n",
    "\n",
    "training_data = dataset[(dataset.season != 2014) & (dataset.season > 2014-4)]\n",
    "testing_data = dataset[dataset.season == 2014]\n",
    "\n",
    "#Preprocess/Filter Data here\n",
    "training_X = training_data[training_data.columns[1:len(training_data.columns)-1]].fillna(0)\n",
    "training_Y = training_data[training_data.columns[-1]]\n",
    "\n",
    "testing_X = testing_data[testing_data.columns[1:len(testing_data.columns)-1]].fillna(0)\n",
    "testing_Y = testing_data[testing_data.columns[-1]]\n",
    "\n",
    "testing_X = training_X\n",
    "testing_Y = training_Y\n",
    "\n",
    "classifiers = [RandomForestClassifier(),LogisticRegression()]\n",
    "prediction_probabilities_for_winning = pd.DataFrame()\n",
    "\n",
    "for classifier in classifiers:\n",
    "    print(classifier.__class__)\n",
    "    predicted_Y, predicted_probabilities_Y = getPredictionsFromClassifier(classifier,training_X,training_Y,testing_X,testing_Y)\n",
    "    prediction_probabilities_for_winning = addPredictionAndClassifierToList(prediction_probabilities_for_winning,classifier,predicted_probabilities_Y)   \n",
    "    \n",
    "training_Y = pd.DataFrame(training_Y.as_matrix(),columns=[\"winningTeam\"])\n",
    "\n",
    "ensemble_training_X = prediction_probabilities_for_winning\n",
    "ensemble_training_Y = training_Y\n",
    "\n",
    "testing_X = testing_data[testing_data.columns[1:len(testing_data.columns)-1]].fillna(0)\n",
    "testing_Y = testing_data[testing_data.columns[-1]]\n",
    "\n",
    "prediction_probabilities_for_winning = pd.DataFrame()\n",
    "\n",
    "for classifier in classifiers:\n",
    "    predicted_Y, predicted_probabilities_Y = getPredictionsFromClassifier(classifier,training_X,training_Y,testing_X,testing_Y)\n",
    "    prediction_probabilities_for_winning = addPredictionAndClassifierToList(prediction_probabilities_for_winning,classifier,predicted_probabilities_Y)   \n",
    "    \n",
    "ensemble_testing_X = prediction_probabilities_for_winning\n",
    "ensemble_testing_Y = testing_Y    \n",
    "\n",
    "ensemble_classifier = RandomForestClassifier()\n",
    "ensemble_classifier.fit(ensemble_training_X,ensemble_training_Y)\n",
    "\n",
    "y_pred = ensemble_classifier.predict(ensemble_testing_X)\n",
    "print(classification_report(y_pred,ensemble_testing_Y))\n",
    "print(accuracy_score(y_pred,ensemble_testing_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:18: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "/usr/local/lib/python2.7/site-packages/sklearn/utils/validation.py:449: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((201, 2), (201, 1), (67, 2), (67,))"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_training_X.shape,ensemble_training_Y.shape,ensemble_testing_X.shape,ensemble_testing_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.59      0.59        37\n",
      "          1       0.48      0.47      0.47        30\n",
      "\n",
      "avg / total       0.54      0.54      0.54        67\n",
      "\n",
      "0.537313432836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/IPython/kernel/__main__.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  from IPython.kernel.zmq import kernelapp as app\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>LogisticRegression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0 </th>\n",
       "      <td> 0.6</td>\n",
       "      <td> 0.657502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 </th>\n",
       "      <td> 0.2</td>\n",
       "      <td> 0.115596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 </th>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0.244128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 </th>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0.655341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 </th>\n",
       "      <td> 0.3</td>\n",
       "      <td> 0.145982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 </th>\n",
       "      <td> 0.7</td>\n",
       "      <td> 0.690461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 </th>\n",
       "      <td> 0.7</td>\n",
       "      <td> 0.964118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7 </th>\n",
       "      <td> 0.3</td>\n",
       "      <td> 0.120040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8 </th>\n",
       "      <td> 0.7</td>\n",
       "      <td> 0.019753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9 </th>\n",
       "      <td> 0.7</td>\n",
       "      <td> 0.203456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td> 0.3</td>\n",
       "      <td> 0.326085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td> 0.7</td>\n",
       "      <td> 0.167646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td> 0.8</td>\n",
       "      <td> 0.161016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0.405809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td> 0.8</td>\n",
       "      <td> 0.902199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0.092343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td> 0.2</td>\n",
       "      <td> 0.017039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td> 0.8</td>\n",
       "      <td> 0.767935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0.096813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td> 0.5</td>\n",
       "      <td> 0.659482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td> 0.3</td>\n",
       "      <td> 0.016967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0.130987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td> 0.7</td>\n",
       "      <td> 0.679349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td> 0.5</td>\n",
       "      <td> 0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td> 0.6</td>\n",
       "      <td> 0.647356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0.789348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td> 0.7</td>\n",
       "      <td> 0.911328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td> 0.5</td>\n",
       "      <td> 0.687886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td> 0.7</td>\n",
       "      <td> 0.231536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td> 0.5</td>\n",
       "      <td> 0.559389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td> 0.7</td>\n",
       "      <td> 0.917593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td> 0.1</td>\n",
       "      <td> 0.044406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td> 0.1</td>\n",
       "      <td> 0.019620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0.166787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td> 0.1</td>\n",
       "      <td> 0.290573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td> 0.6</td>\n",
       "      <td> 0.631266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td> 0.3</td>\n",
       "      <td> 0.506071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td> 0.1</td>\n",
       "      <td> 0.064235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td> 0.5</td>\n",
       "      <td> 0.064438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td> 0.6</td>\n",
       "      <td> 0.739291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td> 0.3</td>\n",
       "      <td> 0.573247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0.235672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td> 0.6</td>\n",
       "      <td> 0.565551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0.748701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td> 0.3</td>\n",
       "      <td> 0.583551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td> 0.5</td>\n",
       "      <td> 0.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td> 0.7</td>\n",
       "      <td> 0.508357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td> 0.7</td>\n",
       "      <td> 0.206212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td> 0.4</td>\n",
       "      <td> 0.174968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td> 0.6</td>\n",
       "      <td> 0.270235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td> 0.6</td>\n",
       "      <td> 0.567365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td> 0.7</td>\n",
       "      <td> 0.548269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td> 0.6</td>\n",
       "      <td> 0.716144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td> 0.7</td>\n",
       "      <td> 0.927480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td> 0.3</td>\n",
       "      <td> 0.085023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td> 0.5</td>\n",
       "      <td> 0.447706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td> 0.5</td>\n",
       "      <td> 0.274093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td> 0.8</td>\n",
       "      <td> 0.935189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td> 0.6</td>\n",
       "      <td> 0.085702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td> 0.6</td>\n",
       "      <td> 0.871322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    RandomForestClassifier  LogisticRegression\n",
       "0                      0.6            0.657502\n",
       "1                      0.2            0.115596\n",
       "2                      0.4            0.244128\n",
       "3                      0.4            0.655341\n",
       "4                      0.3            0.145982\n",
       "5                      0.7            0.690461\n",
       "6                      0.7            0.964118\n",
       "7                      0.3            0.120040\n",
       "8                      0.7            0.019753\n",
       "9                      0.7            0.203456\n",
       "10                     0.3            0.326085\n",
       "11                     0.7            0.167646\n",
       "12                     0.8            0.161016\n",
       "13                     0.4            0.405809\n",
       "14                     0.8            0.902199\n",
       "15                     0.4            0.092343\n",
       "16                     0.2            0.017039\n",
       "17                     0.8            0.767935\n",
       "18                     0.4            0.096813\n",
       "19                     0.5            0.659482\n",
       "20                     0.3            0.016967\n",
       "21                     0.4            0.130987\n",
       "22                     0.7            0.679349\n",
       "23                     0.5            0.683000\n",
       "24                     0.6            0.647356\n",
       "25                     0.4            0.789348\n",
       "26                     0.7            0.911328\n",
       "27                     0.5            0.687886\n",
       "28                     0.7            0.231536\n",
       "29                     0.5            0.559389\n",
       "..                     ...                 ...\n",
       "37                     0.7            0.917593\n",
       "38                     0.1            0.044406\n",
       "39                     0.1            0.019620\n",
       "40                     0.4            0.166787\n",
       "41                     0.1            0.290573\n",
       "42                     0.6            0.631266\n",
       "43                     0.3            0.506071\n",
       "44                     0.1            0.064235\n",
       "45                     0.5            0.064438\n",
       "46                     0.6            0.739291\n",
       "47                     0.3            0.573247\n",
       "48                     0.4            0.235672\n",
       "49                     0.6            0.565551\n",
       "50                     0.4            0.748701\n",
       "51                     0.3            0.583551\n",
       "52                     0.5            0.534000\n",
       "53                     0.7            0.508357\n",
       "54                     0.7            0.206212\n",
       "55                     0.4            0.174968\n",
       "56                     0.6            0.270235\n",
       "57                     0.6            0.567365\n",
       "58                     0.7            0.548269\n",
       "59                     0.6            0.716144\n",
       "60                     0.7            0.927480\n",
       "61                     0.3            0.085023\n",
       "62                     0.5            0.447706\n",
       "63                     0.5            0.274093\n",
       "64                     0.8            0.935189\n",
       "65                     0.6            0.085702\n",
       "66                     0.6            0.871322\n",
       "\n",
       "[67 rows x 2 columns]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_training_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
